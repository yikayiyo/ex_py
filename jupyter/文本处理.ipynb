{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import load_files\n",
    "container_path = r\".\\20newsbydate\\20news-bydate-train\"\n",
    "twenty_train = load_files(container_path=container_path,categories=categories,shuffle=True,random_state=940302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签类别:['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n"
     ]
    }
   ],
   "source": [
    "print('标签类别:%s'%twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'From: zyeh@caspian.usc.edu (zhenghao yeh)\\nSubject: Ellipse Again\\nOrganization: University of Southern California, Los Angeles, CA\\nLines: 39\\nDistribution: world\\nNNTP-Posting-Host: caspian.usc.edu\\nKeywords: ellipse\\n\\n\\nHi! Everyone,\\n\\nBecause no one has touched the problem I posted last week, I guess\\nmy question was not so clear. Now I\\'d like to describe it in detail:\\n\\nThe offset of an ellipse is the locus of the center of a circle which\\nrolls on the ellipse. In other words, the distance between the ellipse\\nand its offset is same everywhere.\\n\\nThis problem comes from the geometric measurement when a probe is used.\\nThe tip of the probe is a ball and the computer just outputs the\\npositions of the ball\\'s center. Is the offset of an ellipse still\\nan ellipse? The answer is no! Ironically, DMIS - an American Indutrial\\nStandard says it is ellipse. So almost all the software which was\\nimplemented on the base of DMIS was wrong. The software was also sold\\ninternationaly. Imagine, how many people have or will suffer from this bug!!!\\nHow many qualified parts with ellipse were/will be discarded? And most\\nimportantly, how many defective parts with ellipse are/will be used?\\n\\nI was employed as a consultant by a company in Los Angeles last year\\nto specially solve this problem. I spent two months on analysis of this\\nproblem and six months on programming. Now my solution (nonlinear)\\nis not ideal because I can only reconstruct an ellipse from its entire\\nor half offset. It is very difficult to find the original ellipse from\\na quarter or a segment of its offset because the method I used is not\\nanalytical. I am now wondering if I didn\\'t touch the base and make things\\ncomplicated. Please give me a hint.\\n\\nI know you may argue this is not a CG problem. You are right, it is not.\\nHowever, so many people involved in the problem \"sphere from 4 poits\".\\nWhy not an ellipse? And why not its offset?\\n\\nPlease post here and let the others share our interests \\n(I got several emails from our netters, they said they need the\\nsummary of the answers).\\n\\nYeh\\nUSC\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 3 2 2 3 2 3 3]\n",
      "1 comp.graphics\n",
      "1 comp.graphics\n",
      "1 comp.graphics\n",
      "3 soc.religion.christian\n",
      "2 sci.med\n",
      "2 sci.med\n",
      "3 soc.religion.christian\n",
      "2 sci.med\n",
      "3 soc.religion.christian\n",
      "3 soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target[:10])\n",
    "for t in twenty_train.target[:10]:\n",
    "    print(t,twenty_train.target_names[t])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词袋模型\n",
    "需要一个词典，大小为VOC\n",
    "\n",
    "对于每一个文档，某词出现的总数作为其特征值，这样每一个文档的向量维度为1*VOC\n",
    "\n",
    "整个数据集全部表示成这样，维度为 n_samples*VOC，占用字节4*n_samples*VOC(如果有10000个文本，词典有100000个词，则占用4GB)\n",
    "\n",
    "注意到文档这样的表示向量是非常稀疏的\n",
    "\n",
    "可以通过只在内存中保存特征向量中非0的部分节省大量内存，scipy.sparse矩阵正是能完成这种操作的数据结构\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词\n",
    "\n",
    "文本的预处理, 分词以及去停用词都被包含在一个高级组件\n",
    "\n",
    "该组件可以构建特征字典和将文档转换成特征向量的\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集维度：(2257, 35787)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vect = CountVectorizer() #有解码错误 UnicodeDecodeError\n",
    "count_vect = CountVectorizer(decode_error='ignore') \n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print('训练集维度：{}'.format(X_train_counts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm') #文档集中出现的总次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小：35787\n"
     ]
    }
   ],
   "source": [
    "print('字典大小：%d'%len(count_vect.vocabulary_) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从次数到频率\n",
    "词频TF\n",
    "\n",
    "逆文档频率IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35787)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练分类器\n",
    "**多分类任务**\n",
    "\n",
    "先来一个SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "# 不需要fit，直接transform\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(decode_error='ignore')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83488681757656458"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_path = r\".\\20newsbydate\\20news-bydate-test\"\n",
    "twenty_test = load_files(container_path=test_path,\n",
    "    categories=categories, shuffle=True, random_state=940302)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更换分类器 比较结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91211717709720375"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_text_clf = Pipeline([('vect', CountVectorizer(decode_error='ignore')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42,\n",
    "                                           max_iter=5, tol=None)),\n",
    "])\n",
    "sgd_text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "sgd_predicted = sgd_text_clf.predict(docs_test)\n",
    "np.mean(sgd_predicted == twenty_test.target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "           avg / total       0.88      0.83      0.84      1502\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.87      0.98      0.92       389\n",
      "               sci.med       0.95      0.89      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "           avg / total       0.92      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(twenty_test.target, predicted,target_names=twenty_test.target_names))\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print(metrics.classification_report(twenty_test.target, sgd_predicted,target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[192   2   6 119]\n",
      " [  2 347   4  36]\n",
      " [  2  11 322  61]\n",
      " [  2   2   1 393]]\n",
      "-------------------------------------------------------\n",
      "[[258  11  14  36]\n",
      " [  4 381   2   2]\n",
      " [  4  36 353   3]\n",
      " [  5  11   4 378]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(twenty_test.target, predicted))\n",
    "print('-------------------------------------------------------')\n",
    "print(metrics.confusion_matrix(twenty_test.target, sgd_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91944074567243672"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_text_clf = Pipeline([('vect', CountVectorizer(decode_error='ignore')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MLPClassifier(alpha=0.1,random_state=940302))\n",
    "])\n",
    "mlp_text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "mlp_predicted = mlp_text_clf.predict(docs_test)\n",
    "np.mean(mlp_predicted == twenty_test.target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------MLP------------------------------\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.83      0.89       319\n",
      "         comp.graphics       0.90      0.96      0.93       389\n",
      "               sci.med       0.94      0.91      0.93       396\n",
      "soc.religion.christian       0.90      0.96      0.93       398\n",
      "\n",
      "           avg / total       0.92      0.92      0.92      1502\n",
      "\n",
      "------------------------------------------------------\n",
      "[[264   8  14  33]\n",
      " [  4 374   6   5]\n",
      " [  5  25 362   4]\n",
      " [  4  10   3 381]]\n"
     ]
    }
   ],
   "source": [
    "print('------------------------MLP------------------------------')\n",
    "print(metrics.classification_report(twenty_test.target, mlp_predicted,target_names=twenty_test.target_names))\n",
    "print('------------------------------------------------------')\n",
    "print(metrics.confusion_matrix(twenty_test.target, mlp_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch\n",
    "参数搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([ 0.11272264,  0.41321683,  0.09808366,  0.37499539,  0.1014092 ,\n",
      "        0.33843939,  0.09970959,  0.34205898]), 'std_fit_time': array([ 0.03384394,  0.02657769,  0.00783809,  0.00646191,  0.00965604,\n",
      "        0.02973199,  0.00454739,  0.02653395]), 'mean_score_time': array([ 0.03954943,  0.12366796,  0.03623621,  0.0777936 ,  0.03324254,\n",
      "        0.07877556,  0.0378983 ,  0.07247313]), 'std_score_time': array([ 0.00589848,  0.04292752,  0.00204914,  0.00373202,  0.00248752,\n",
      "        0.00666066,  0.00564296,  0.00308289]), 'param_clf__alpha': masked_array(data = [0.01 0.01 0.01 0.01 0.001 0.001 0.001 0.001],\n",
      "             mask = [False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False],\n",
      "             mask = [False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)],\n",
      "             mask = [False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'params': [{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}], 'split0_test_score': array([ 0.94029851,  0.91791045,  0.90298507,  0.91044776,  0.95522388,\n",
      "        0.92537313,  0.94029851,  0.93283582]), 'split1_test_score': array([ 0.90977444,  0.90225564,  0.89473684,  0.87969925,  0.92481203,\n",
      "        0.90225564,  0.91729323,  0.92481203]), 'split2_test_score': array([ 0.94736842,  0.91729323,  0.93233083,  0.89473684,  0.93984962,\n",
      "        0.90977444,  0.93233083,  0.90977444]), 'mean_test_score': array([ 0.9325,  0.9125,  0.91  ,  0.895 ,  0.94  ,  0.9125,  0.93  ,\n",
      "        0.9225]), 'std_test_score': array([ 0.01629722,  0.00723467,  0.01611682,  0.01256216,  0.01242371,\n",
      "        0.00963744,  0.00954056,  0.00956087]), 'rank_test_score': array([2, 5, 7, 8, 1, 5, 3, 4]), 'split0_train_score': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), 'split1_train_score': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), 'split2_train_score': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), 'mean_train_score': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), 'std_train_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])}\n",
      "==========================\n",
      "NB 最佳分数：0.94\n",
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# 参数里包含pipe各个步骤的\n",
    "# 形式为：步骤名+'__'+参数名\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf,parameters)\n",
    "# 使用少量数据来加快得出结果\n",
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])\n",
    "print(gs_clf.cv_results_)\n",
    "print('==========================')\n",
    "print('NB 最佳分数：{}'.format(gs_clf.best_score_))                                  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([ 0.55257718,  1.97323767,  0.4883062 ,  1.8908004 ,  0.46863023,\n",
      "        1.79583661,  0.47405513,  1.76170047,  0.51560243,  1.9744308 ,\n",
      "        0.49653546,  1.9365259 ,  0.48456621,  1.81266443,  0.46165156,\n",
      "        1.89812128,  0.53225549,  2.00030406,  0.52555895,  1.94912092,\n",
      "        0.48234979,  1.8696537 ,  0.48069016,  1.79186384,  0.52889562,\n",
      "        2.02457309,  0.52422865,  1.96006632,  0.48934587,  1.85501297,\n",
      "        0.48036933,  1.97956546]), 'std_fit_time': array([ 0.04281144,  0.08071552,  0.02614348,  0.07771937,  0.02550959,\n",
      "        0.07677181,  0.02669706,  0.05397623,  0.02195435,  0.09297362,\n",
      "        0.02014624,  0.07601339,  0.01280263,  0.06625402,  0.02128854,\n",
      "        0.08085693,  0.03622335,  0.0917609 ,  0.00746391,  0.09223856,\n",
      "        0.0057797 ,  0.08221593,  0.01820333,  0.07249273,  0.02032907,\n",
      "        0.06914565,  0.01968573,  0.08305312,  0.01823528,  0.06434113,\n",
      "        0.02078503,  0.19599444]), 'mean_score_time': array([ 0.27730727,  0.45835257,  0.19786938,  0.43761094,  0.20828303,\n",
      "        0.46343199,  0.22390437,  0.44792048,  0.20848815,  0.47396151,\n",
      "        0.19787018,  0.43508903,  0.1978666 ,  0.48016715,  0.23431738,\n",
      "        0.45326447,  0.20878887,  0.47739291,  0.21775174,  0.4594396 ,\n",
      "        0.20911002,  0.46542446,  0.20743521,  0.4527909 ,  0.20677996,\n",
      "        0.47340298,  0.20678123,  0.46376316,  0.21674252,  0.46675444,\n",
      "        0.19813816,  0.45445053]), 'std_score_time': array([ 0.08657666,  0.01965301,  0.01472774,  0.03382814,  0.0194834 ,\n",
      "        0.01472937,  0.01948151,  0.03674694,  0.00721999,  0.02940079,\n",
      "        0.007363  ,  0.03054234,  0.01472869,  0.0381626 ,  0.02550773,\n",
      "        0.02379394,  0.00556032,  0.03462827,  0.0124386 ,  0.01892911,\n",
      "        0.01186558,  0.02601166,  0.00862434,  0.02690847,  0.02168551,\n",
      "        0.02444753,  0.00863076,  0.03186203,  0.01339973,  0.02850142,\n",
      "        0.01265021,  0.02282248]), 'param_clf__alpha': masked_array(data = [0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001\n",
      " 0.001 0.001 0.001 0.001 0.001],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_clf__loss': masked_array(data = ['hinge' 'hinge' 'hinge' 'hinge' 'hinge' 'hinge' 'hinge' 'hinge'\n",
      " 'modified_huber' 'modified_huber' 'modified_huber' 'modified_huber'\n",
      " 'modified_huber' 'modified_huber' 'modified_huber' 'modified_huber'\n",
      " 'hinge' 'hinge' 'hinge' 'hinge' 'hinge' 'hinge' 'hinge' 'hinge'\n",
      " 'modified_huber' 'modified_huber' 'modified_huber' 'modified_huber'\n",
      " 'modified_huber' 'modified_huber' 'modified_huber' 'modified_huber'],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_clf__penalty': masked_array(data = ['l1' 'l1' 'l1' 'l1' 'l2' 'l2' 'l2' 'l2' 'l1' 'l1' 'l1' 'l1' 'l2' 'l2' 'l2'\n",
      " 'l2' 'l1' 'l1' 'l1' 'l1' 'l2' 'l2' 'l2' 'l2' 'l1' 'l1' 'l1' 'l1' 'l2' 'l2'\n",
      " 'l2' 'l2'],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False True True False False True\n",
      " True False False True True False False True True False False True True\n",
      " False False True True False False],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)\n",
      " (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)\n",
      " (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)\n",
      " (1, 1) (1, 2)],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'params': [{'clf__alpha': 0.01, 'clf__loss': 'hinge', 'clf__penalty': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'clf__loss': 'hinge', 'clf__penalty': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.01, 'clf__loss': 'hinge', 'clf__penalty': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'clf__loss': 'hinge', 'clf__penalty': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.01, 'clf__loss': 'hinge', 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'clf__loss': 'hinge', 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.01, 'clf__loss': 'hinge', 'clf__penalty': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'clf__loss': 'hinge', 'clf__penalty': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.01, 'clf__loss': 'modified_huber', 'clf__penalty': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'clf__loss': 'modified_huber', 'clf__penalty': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.01, 'clf__loss': 'modified_huber', 'clf__penalty': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'clf__loss': 'modified_huber', 'clf__penalty': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.01, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.01, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.01, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'clf__loss': 'hinge', 'clf__penalty': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'clf__loss': 'hinge', 'clf__penalty': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'clf__loss': 'hinge', 'clf__penalty': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'clf__loss': 'hinge', 'clf__penalty': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'clf__loss': 'hinge', 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'clf__loss': 'hinge', 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'clf__loss': 'hinge', 'clf__penalty': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'clf__loss': 'hinge', 'clf__penalty': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}, {'clf__alpha': 0.001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}, {'clf__alpha': 0.001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}], 'split0_test_score': array([ 0.33466135,  0.30810093,  0.47144754,  0.43027888,  0.88844622,\n",
      "        0.90039841,  0.81142098,  0.81938911,  0.60292165,  0.52988048,\n",
      "        0.60823373,  0.57901726,  0.91633466,  0.90836653,  0.8313413 ,\n",
      "        0.84063745,  0.87782205,  0.812749  ,  0.81540505,  0.78486056,\n",
      "        0.95351926,  0.94156707,  0.90172643,  0.92031873,  0.91633466,\n",
      "        0.90305445,  0.88446215,  0.87782205,  0.96015936,  0.95351926,\n",
      "        0.93359894,  0.94289509]), 'split1_test_score': array([ 0.31606906,  0.30146082,  0.44090305,  0.41699867,  0.90571049,\n",
      "        0.9309429 ,  0.81540505,  0.84329349,  0.63612218,  0.51128818,\n",
      "        0.6002656 ,  0.56440903,  0.93359894,  0.92695883,  0.84727756,\n",
      "        0.85790173,  0.90039841,  0.81938911,  0.82735724,  0.79548473,\n",
      "        0.97078353,  0.96812749,  0.93492696,  0.94156707,  0.94422311,\n",
      "        0.91766268,  0.90039841,  0.87118194,  0.97343958,  0.96945551,\n",
      "        0.94953519,  0.94953519]), 'split2_test_score': array([ 0.3954727 ,  0.26498003,  0.44740346,  0.42743009,  0.89347537,\n",
      "        0.91877497,  0.80159787,  0.83621838,  0.58988016,  0.50599201,\n",
      "        0.6138482 ,  0.55925433,  0.91344874,  0.9081225 ,  0.85219707,\n",
      "        0.8482024 ,  0.88681758,  0.83888149,  0.82157124,  0.7976032 ,\n",
      "        0.95339547,  0.95339547,  0.90146471,  0.90679095,  0.92010652,\n",
      "        0.90545939,  0.89613848,  0.87882823,  0.95739015,  0.95739015,\n",
      "        0.9241012 ,  0.92942743]), 'mean_test_score': array([ 0.34869296,  0.29153744,  0.45325654,  0.42490031,  0.89587949,\n",
      "        0.91670359,  0.80948161,  0.83296411,  0.60965884,  0.51572884,\n",
      "        0.60744351,  0.56756757,  0.92113425,  0.91448826,  0.8435977 ,\n",
      "        0.84891449,  0.88834736,  0.82365973,  0.8214444 ,  0.7926451 ,\n",
      "        0.95923793,  0.9543642 ,  0.91271599,  0.92290651,  0.92689411,\n",
      "        0.9087284 ,  0.89366416,  0.87594152,  0.96366859,  0.96012406,\n",
      "        0.93575543,  0.94062915]), 'std_test_score': array([ 0.03389586,  0.01894905,  0.01314214,  0.00571064,  0.00725273,\n",
      "        0.01256072,  0.00580017,  0.01003006,  0.01946674,  0.01024404,\n",
      "        0.00557196,  0.00837027,  0.00889803,  0.00882445,  0.00890176,\n",
      "        0.00706914,  0.00928391,  0.01108595,  0.00488244,  0.00557562,\n",
      "        0.00816956,  0.0108696 ,  0.01571633,  0.01431175,  0.01235787,\n",
      "        0.00639744,  0.00673928,  0.00339271,  0.00700551,  0.00678912,\n",
      "        0.01049262,  0.00836228]), 'rank_test_score': array([31, 32, 29, 30, 14, 10, 23, 20, 25, 28, 26, 27,  9, 11, 19, 18, 16,\n",
      "       21, 22, 24,  3,  4, 12,  8,  7, 13, 15, 17,  1,  2,  6,  5]), 'split0_train_score': array([ 0.31914894,  0.30718085,  0.47140957,  0.43683511,  0.93949468,\n",
      "        0.96808511,  0.85505319,  0.87632979,  0.62101064,  0.51795213,\n",
      "        0.62965426,  0.57513298,  0.97406915,  0.98071809,  0.90625   ,\n",
      "        0.92619681,  0.90558511,  0.82912234,  0.86236702,  0.81914894,\n",
      "        0.99867021,  0.99867021,  0.96542553,  0.98404255,  0.97007979,\n",
      "        0.93882979,  0.96143617,  0.94481383,  0.99933511,  0.99933511,\n",
      "        0.99667553,  0.99933511]), 'split1_train_score': array([ 0.33577128,  0.3125    ,  0.45013298,  0.43218085,  0.93617021,\n",
      "        0.97207447,  0.85970745,  0.88297872,  0.61103723,  0.50864362,\n",
      "        0.60837766,  0.57180851,  0.97074468,  0.9787234 ,  0.90691489,\n",
      "        0.92154255,  0.88962766,  0.81781915,  0.84973404,  0.81648936,\n",
      "        0.99867021,  0.99867021,  0.97273936,  0.98869681,  0.96476064,\n",
      "        0.9268617 ,  0.94215426,  0.92021277,  0.99933511,  0.99933511,\n",
      "        0.99667553,  0.99933511]), 'split2_train_score': array([ 0.40371846,  0.26560425,  0.46082337,  0.43227092,  0.95418327,\n",
      "        0.98007968,  0.86188579,  0.89508632,  0.61553785,  0.5185923 ,\n",
      "        0.62151394,  0.58499336,  0.97808765,  0.98273572,  0.91102258,\n",
      "        0.92695883,  0.91301461,  0.84063745,  0.85989376,  0.82802125,\n",
      "        0.99867198,  1.        ,  0.96480744,  0.9814077 ,  0.97211155,\n",
      "        0.94289509,  0.95484728,  0.93957503,  1.        ,  1.        ,\n",
      "        0.99136786,  0.99933599]), 'mean_train_score': array([ 0.35287956,  0.29509503,  0.46078864,  0.43376229,  0.94328272,\n",
      "        0.97341309,  0.85888214,  0.88479828,  0.61586191,  0.51506268,\n",
      "        0.61984862,  0.57731162,  0.97430049,  0.98072574,  0.90806249,\n",
      "        0.9248994 ,  0.90274246,  0.82919298,  0.85733161,  0.82121985,\n",
      "        0.9986708 ,  0.99911348,  0.96765744,  0.98471569,  0.96898399,\n",
      "        0.93619553,  0.95281257,  0.93486721,  0.99955674,  0.99955674,\n",
      "        0.99490631,  0.9993354 ]), 'std_train_score': array([  3.65834300e-02,   2.09658943e-02,   8.68616856e-03,\n",
      "         2.17311950e-03,   7.82642795e-03,   4.98740939e-03,\n",
      "         2.84978902e-03,   7.76466250e-03,   4.07806805e-03,\n",
      "         4.54648131e-03,   8.76559034e-03,   5.59880399e-03,\n",
      "         3.00221439e-03,   1.63803152e-03,   2.11062461e-03,\n",
      "         2.39394701e-03,   9.75697457e-03,   9.31566640e-03,\n",
      "         5.46635167e-03,   4.93035573e-03,   8.32493644e-07,\n",
      "         6.26867714e-04,   3.60230757e-03,   3.01359137e-03,\n",
      "         3.09941534e-03,   6.80548085e-03,   8.00221193e-03,\n",
      "         1.05806675e-02,   3.13433857e-04,   3.13433857e-04,\n",
      "         2.50205965e-03,   4.16246822e-07])}\n",
      "==========================\n",
      "SGD 最佳分数：0.9636685866194062\n",
      "clf__alpha: 0.001\n",
      "clf__loss: 'modified_huber'\n",
      "clf__penalty: 'l2'\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "              'clf__loss':('hinge','modified_huber'),\n",
    "              'clf__penalty':('l1','l2'),            \n",
    "}\n",
    "gs_sgd_clf = GridSearchCV(sgd_text_clf,parameters)\n",
    "gs_sgd_clf = gs_sgd_clf.fit(twenty_train.data, twenty_train.target)\n",
    "print(gs_sgd_clf.cv_results_)\n",
    "print('==========================')\n",
    "print('SGD 最佳分数：{}'.format(gs_sgd_clf.best_score_))                                  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_sgd_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gao\\software\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\gao\\software\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\gao\\software\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "              'clf__activation':('relu','tanh'),\n",
    "              'clf__solver':('sgd','adam')\n",
    "}\n",
    "gs_mlp_clf = GridSearchCV(mlp_text_clf,parameters)\n",
    "gs_mlp_clf = gs_mlp_clf.fit(twenty_train.data, twenty_train.target)\n",
    "print(gs_mlp_clf.cv_results_)\n",
    "print('==========================')\n",
    "print('MLP 最佳分数：{}'.format(gs_mlp_clf.best_score_))                                  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_mlp_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
